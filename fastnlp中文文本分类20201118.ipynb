{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fastNLP\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 382688/382688 [00:00<00:00, 400528.36it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data/toutiao_news_dataset.txt\"\n",
    "with open(data_path, \"r\", encoding=\"utf8\") as fo:\n",
    "        all_lines = fo.readlines()\n",
    "datas, labels = [], []\n",
    "for line in tqdm(all_lines):\n",
    "    content_words = []\n",
    "    category, content = line.strip().split(\"_!_\")\n",
    "#     for term in HanLP.segment(content):\n",
    "#         if term.word not in word_vocabs:\n",
    "#             word_vocabs[term.word] = len(word_vocabs)+1\n",
    "#         content_words.append(term.word)\n",
    "    datas.append(content)\n",
    "    labels.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382688"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382688"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target= lb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 7, 7, ..., 3, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['体育', '军事', '农业', '国际', '娱乐', '房产', '教育', '文化', '旅游', '民生故事', '汽车',\n",
       "       '电竞游戏', '科技', '证券股票', '财经'], dtype='<U4')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastNLP import DataSet\n",
    "from fastNLP.io import DataBundle\n",
    "from fastNLP import Instance\n",
    "from fastNLP import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_train=DataSet()\n",
    "# data_test=DataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds=DataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(datas[:2000])):\n",
    "#         title=title_texts[i]\n",
    "#         context=context_texts[i]\n",
    "        question=datas[i]\n",
    "        answers=target[i]\n",
    "#         answer_starts=answer_starts_texts\n",
    "#         id=id_texts\n",
    "        ds.append(Instance(text=question, label=answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In total 2 datasets:\n",
       "\ttrain has 1600 instances.\n",
       "\tdev has 400 instances."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bundle=DataBundle()\n",
    "train_data, dev_data = ds.split(0.2, shuffle=False)\n",
    "data_bundle.set_dataset(train_data, 'train')\n",
    "data_bundle.set_dataset(dev_data, 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'dev']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bundle.get_dataset_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In total 2 datasets:\n",
       "\ttrain has 1600 instances.\n",
       "\tdev has 400 instances."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bundle.apply(lambda x: int(x['label']),new_field_name='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In total 2 datasets:\n",
       "\ttrain has 1600 instances.\n",
       "\tdev has 400 instances."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bundle.apply(lambda x: x['text'], new_field_name='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-------------------------------------+-------+\n",
       "| text                                | label |\n",
       "+-------------------------------------+-------+\n",
       "| 京城最值得你来场文化之旅的博物馆... | 7     |\n",
       "| 发酵床的垫料种类有哪些？哪种更好... | 7     |\n",
       "| 上联：黄山黄河黄皮肤黄土高原。怎... | 7     |\n",
       "| 林徽因什么理由拒绝了徐志摩而选择... | 7     |\n",
       "| 黄杨木是什么树？                    | 7     |\n",
       "| 上联：草根登上星光道，怎么对下联... | 7     |\n",
       "| 什么是超写实绘画？                  | 7     |\n",
       "| 松涛听雨莺婉转，下联？              | 7     |\n",
       "| 上联：老子骑牛读书，下联怎么对？... | 7     |\n",
       "| 上联：山水醉人何须酒。如何对下联... | 7     |\n",
       "| 国画山水，如何读懂山水画            | 7     |\n",
       "| 一元硬币是这种，现在价值24000元...  | 7     |\n",
       "| 有哪些让人感动的语句呢？            | 7     |\n",
       "| 上联，绿竹引清风，如何对下联？      | 7     |\n",
       "| 赵宁安作品欣赏                      | 7     |\n",
       "| 夕阳无语燕归愁，如何接下句？        | 7     |\n",
       "| 上联：山水醉人何须酒。如何对下联... | 7     |\n",
       "| 上联：上班为下班，如何对下联？      | 7     |\n",
       "| 下联:夕陽西下已黄昏。上联是什麽...  | 7     |\n",
       "| 初夏方好，小荷初露，一起来读那些... | 7     |\n",
       "| 谢娜为李浩菲澄清网络谣言，之后她... | 4     |\n",
       "| 谢娜曾为他与主办方撕破脸！谢娜复... | 4     |\n",
       "| 中国网红竟红到美国？不多说了，连... | 4     |\n",
       "| 赵丽颖很久没有登上微博热搜了，但... | 4     |\n",
       "| 因戴一个眼镜更改变气质的6大娱乐...  | 4     |\n",
       "| 后来的我们，抢先看                  | 4     |\n",
       "| 超级英雄演员颜值身材排名，钢铁侠... | 4     |\n",
       "| 《无限歌谣季》热播 张绍刚毛不易...  | 4     |\n",
       "| 张靓颖透露右耳已经间歇性失聪10年... | 4     |\n",
       "| 成龙改口决定不裸捐了，20亿财产给... | 4     |\n",
       "| 五大“出轨“女明星，最后一个你们...   | 4     |\n",
       "| ...                                 | ...   |\n",
       "+-------------------------------------+-------+"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bundle.get_dataset(name='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_sent(instance):\n",
    "    return jieba.lcut(instance['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 2.218 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "In total 2 datasets:\n",
       "\ttrain has 1600 instances.\n",
       "\tdev has 400 instances."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bundle.apply(split_sent,new_field_name='description_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+---------------------------+-------+---------------------------+\n",
       "| text                      | label | description_words         |\n",
       "+---------------------------+-------+---------------------------+\n",
       "| 京城最值得你来场文化之... | 7     | ['京城', '最', '值得'...  |\n",
       "| 发酵床的垫料种类有哪些... | 7     | ['发酵', '床', '的', ...  |\n",
       "| 上联：黄山黄河黄皮肤黄... | 7     | ['上联', '：', '黄山'...  |\n",
       "| 林徽因什么理由拒绝了徐... | 7     | ['林徽因', '什么', '理... |\n",
       "| 黄杨木是什么树？          | 7     | ['黄杨木', '是', '什么... |\n",
       "| 上联：草根登上星光道，... | 7     | ['上联', '：', '草根'...  |\n",
       "| 什么是超写实绘画？        | 7     | ['什么', '是', '超', ...  |\n",
       "| 松涛听雨莺婉转，下联？... | 7     | ['松涛', '听雨莺', '婉... |\n",
       "| 上联：老子骑牛读书，下... | 7     | ['上联', '：', '老子'...  |\n",
       "| 上联：山水醉人何须酒。... | 7     | ['上联', '：', '山水'...  |\n",
       "| 国画山水，如何读懂山水... | 7     | ['国画', '山水', '，'...  |\n",
       "| 一元硬币是这种，现在价... | 7     | ['一元', '硬币', '是'...  |\n",
       "| 有哪些让人感动的语句呢... | 7     | ['有', '哪些', '让', ...  |\n",
       "| 上联，绿竹引清风，如何... | 7     | ['上联', '，', '绿竹'...  |\n",
       "| 赵宁安作品欣赏            | 7     | ['赵', '宁安', '作品'...  |\n",
       "| 夕阳无语燕归愁，如何接... | 7     | ['夕阳', '无语', '燕归... |\n",
       "| 上联：山水醉人何须酒。... | 7     | ['上联', '：', '山水'...  |\n",
       "| 上联：上班为下班，如何... | 7     | ['上联', '：', '上班'...  |\n",
       "| 下联:夕陽西下已黄昏。...  | 7     | ['下联', ':', '夕陽',...  |\n",
       "| 初夏方好，小荷初露，一... | 7     | ['初夏', '方好', '，'...  |\n",
       "| 谢娜为李浩菲澄清网络谣... | 4     | ['谢娜', '为', '李浩菲... |\n",
       "| 谢娜曾为他与主办方撕破... | 4     | ['谢娜', '曾', '为', ...  |\n",
       "| 中国网红竟红到美国？不... | 4     | ['中国', '网红竟', '红... |\n",
       "| 赵丽颖很久没有登上微博... | 4     | ['赵丽颖', '很久没', ...  |\n",
       "| 因戴一个眼镜更改变气质... | 4     | ['因戴', '一个', '眼镜... |\n",
       "| 后来的我们，抢先看        | 4     | ['后来', '的', '我们'...  |\n",
       "| 超级英雄演员颜值身材排... | 4     | ['超级', '英雄', '演员... |\n",
       "| 《无限歌谣季》热播 张...  | 4     | ['《', '无限', '歌谣'...  |\n",
       "| 张靓颖透露右耳已经间歇... | 4     | ['张靓颖', '透露', '右... |\n",
       "| 成龙改口决定不裸捐了，... | 4     | ['成龙', '改口', '决定... |\n",
       "| 五大“出轨“女明星，最...   | 4     | ['五大', '“', '出轨'...   |\n",
       "| ...                       | ...   | ...                       |\n",
       "+---------------------------+-------+---------------------------+"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bundle.get_dataset(name='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In total 2 datasets:\n",
       "\ttrain has 1600 instances.\n",
       "\tdev has 400 instances."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bundle.apply(lambda x: len(x['description_words']),new_field_name='description_seq_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_seq_len_train=0\n",
    "max_seq_len_test=0\n",
    "data_train=data_bundle.get_dataset('train')\n",
    "data_test=data_bundle.get_dataset('dev')\n",
    "for i in range (len(data_train)):\n",
    "    if(data_train[i]['description_seq_len'] > max_seq_len_train):\n",
    "        max_seq_len_train = data_train[i]['description_seq_len']\n",
    "    else:\n",
    "        pass\n",
    "for i in range (len(data_test)):\n",
    "    if(data_test[i]['description_seq_len'] > max_seq_len_test):\n",
    "        max_seq_len_test = data_test[i]['description_seq_len']\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sentence_length: 32\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length = max_seq_len_train\n",
    "if (max_seq_len_test > max_sentence_length):\n",
    "    max_sentence_length = max_seq_len_test\n",
    "# max_sentence_length=177\n",
    "print ('max_sentence_length:',max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In total 2 datasets:\n",
       "\ttrain has 1600 instances.\n",
       "\tdev has 400 instances."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bundle.set_input(\"description_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In total 2 datasets:\n",
       "\ttrain has 1600 instances.\n",
       "\tdev has 400 instances."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bundle.set_target(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------------------+-------+------------------------+---------------------+\n",
       "| text                 | label | description_words      | description_seq_len |\n",
       "+----------------------+-------+------------------------+---------------------+\n",
       "| 京城最值得你来场...  | 7     | ['京城', '最', '值...  | 9                   |\n",
       "| 发酵床的垫料种类...  | 7     | ['发酵', '床', '的...  | 12                  |\n",
       "| 上联：黄山黄河黄...  | 7     | ['上联', '：', '黄...  | 11                  |\n",
       "| 林徽因什么理由拒...  | 7     | ['林徽因', '什么',...  | 12                  |\n",
       "| 黄杨木是什么树？...  | 7     | ['黄杨木', '是', '...  | 5                   |\n",
       "| 上联：草根登上星...  | 7     | ['上联', '：', '草...  | 11                  |\n",
       "| 什么是超写实绘画...  | 7     | ['什么', '是', '超...  | 6                   |\n",
       "| 松涛听雨莺婉转，...  | 7     | ['松涛', '听雨莺',...  | 6                   |\n",
       "| 上联：老子骑牛读...  | 7     | ['上联', '：', '老...  | 10                  |\n",
       "| 上联：山水醉人何...  | 7     | ['上联', '：', '山...  | 11                  |\n",
       "| 国画山水，如何读...  | 7     | ['国画', '山水', '...  | 6                   |\n",
       "| 一元硬币是这种，...  | 7     | ['一元', '硬币', '...  | 12                  |\n",
       "| 有哪些让人感动的...  | 7     | ['有', '哪些', '让...  | 9                   |\n",
       "| 上联，绿竹引清风...  | 7     | ['上联', '，', '绿...  | 10                  |\n",
       "| 赵宁安作品欣赏       | 7     | ['赵', '宁安', '作...  | 4                   |\n",
       "| 夕阳无语燕归愁，...  | 7     | ['夕阳', '无语', '...  | 8                   |\n",
       "| 上联：山水醉人何...  | 7     | ['上联', '：', '山...  | 11                  |\n",
       "| 上联：上班为下班...  | 7     | ['上联', '：', '上...  | 10                  |\n",
       "| 下联:夕陽西下已黄... | 7     | ['下联', ':', '夕陽... | 12                  |\n",
       "| 初夏方好，小荷初...  | 7     | ['初夏', '方好', '...  | 16                  |\n",
       "| 谢娜为李浩菲澄清...  | 4     | ['谢娜', '为', '李...  | 16                  |\n",
       "| 谢娜曾为他与主办...  | 4     | ['谢娜', '曾', '为...  | 17                  |\n",
       "| 中国网红竟红到美...  | 4     | ['中国', '网红竟',...  | 13                  |\n",
       "| 赵丽颖很久没有登...  | 4     | ['赵丽颖', '很久没...  | 18                  |\n",
       "| 因戴一个眼镜更改...  | 4     | ['因戴', '一个', '...  | 17                  |\n",
       "| 后来的我们，抢先...  | 4     | ['后来', '的', '我...  | 6                   |\n",
       "| 超级英雄演员颜值...  | 4     | ['超级', '英雄', '...  | 16                  |\n",
       "| 《无限歌谣季》热...  | 4     | ['《', '无限', '歌...  | 15                  |\n",
       "| 张靓颖透露右耳已...  | 4     | ['张靓颖', '透露',...  | 18                  |\n",
       "| 成龙改口决定不裸...  | 4     | ['成龙', '改口', '...  | 18                  |\n",
       "| 五大“出轨“女明...    | 4     | ['五大', '“', '出...   | 13                  |\n",
       "| ...                  | ...   | ...                    | ...                 |\n",
       "+----------------------+-------+------------------------+---------------------+"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bundle.get_dataset('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train=data_bundle.get_dataset(name='train')\n",
    "data_test=data_bundle.get_dataset(name='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary(['高通', '骁龙', '845', '和', '835']...)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocabulary(min_freq=2)\n",
    "data_train.apply(lambda x:[vocab.add(word) for word in x['description_words']])\n",
    "vocab.build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In total 2 datasets:\n",
       "\ttrain has 1600 instances.\n",
       "\tdev has 400 instances.\n",
       "In total 1 vocabs:\n",
       "\tvocab has 8197 entries."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.from_dataset(*[ds for name, ds in data_bundle.iter_datasets() \n",
    "                         if 'train' in name],\n",
    "                       field_name='description_words',\n",
    "                       no_create_entry_dataset=[ds for name\n",
    "                                                , ds in data_bundle.iter_datasets()\n",
    "                                                if 'train' not in name]\n",
    "                       )\n",
    "vocab.index_dataset(*data_bundle.datasets.values()\n",
    "                        , field_name='description_words', new_field_name='description_words')\n",
    "data_bundle.set_vocab(vocab, 'vocab')\n",
    "# for name in list(data_bundle.datasets.keys()):\n",
    "#     ds = data_bundle.get_dataset(name)\n",
    "#     data_bundle.delete_dataset(name)\n",
    "#     new_ds = DataSet()\n",
    "#     # i=0\n",
    "#     for ins in ds:\n",
    "#         new_ins=Instance()\n",
    "#         text = ins['text']\n",
    "#         label = ins['label']\n",
    "#         description_words=ins['description_words']\n",
    "#         description_seq_len=ins['description_seq_len']\n",
    "# #         src_tokens=ins['src_tokens']\n",
    "# #         tgt_tokens=ins['tgt_tokens']\n",
    "#         new_ins['text']=text\n",
    "#         new_ins['label']=label\n",
    "#         new_ins['description_words'] = description_words\n",
    "#         new_ins['description_seq_len'] = description_seq_len\n",
    "# #         new_ins['src_tokens'] = src_tokens\n",
    "# #         new_ins['tgt_tokens'] = tgt_tokens\n",
    "# #         new_ins['src_seq_len'] = len(q_lst)\n",
    "# #         new_ins['tgt_seq_len'] = len(cnt_lst)\n",
    "\n",
    "#         new_ds.append(new_ins)\n",
    "#     data_bundle.set_dataset(new_ds, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train=data_bundle.get_dataset(name='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------+-----------------------+---------------------+\n",
      "| text                 | label | description_words     | description_seq_len |\n",
      "+----------------------+-------+-----------------------+---------------------+\n",
      "| 高通骁龙845和835...  | 12    | [1471, 2727, 2728,... | 12                  |\n",
      "| 百度和谷歌的差距...  | 12    | [977, 23, 1473, 4,... | 8                   |\n",
      "| 三进制计算机可以...  | 12    | [267, 2730, 1474, ... | 7                   |\n",
      "| 国内投资3000亿用...  | 12    | [396, 143, 709, 15... | 19                  |\n",
      "| 这个电脑配置值多...  | 12    | [107, 2734, 1475, ... | 6                   |\n",
      "| 美团众包以后会消...  | 12    | [2735, 2736, 268, ... | 7                   |\n",
      "| 新零售背景下，电...  | 12    | [115, 2737, 2738, ... | 12                  |\n",
      "| 微信打不开了怎么...  | 12    | [155, 2739, 7, 116... | 5                   |\n",
      "| 腾讯发布“00后画...   | 12    | [208, 457, 14, 981... | 16                  |\n",
      "| 持股比例仅次于雷...  | 12    | [2743, 2744, 2745,... | 12                  |\n",
      "| 玉米种子在播前，...  | 12    | [2749, 12, 2750, 2... | 19                  |\n",
      "| 淘宝购物大家能接...  | 12    | [712, 2751, 209, 4... | 11                  |\n",
      "| 电脑挂机软件是真...  | 12    | [713, 2753, 572, 1... | 7                   |\n",
      "| 索尼哪一款手机最...  | 12    | [2754, 158, 573, 1... | 7                   |\n",
      "| 力哥爆料：如果有...  | 12    | [2755, 984, 6, 28,... | 18                  |\n",
      "| 如果所有银行不让...  | 12    | [28, 459, 225, 148... | 13                  |\n",
      "| 军演时来敌人了怎...  | 1     | [987, 1482, 1483, ... | 6                   |\n",
      "| 女乒今天排兵布阵...  | 1     | [988, 460, 1484, 1... | 21                  |\n",
      "| 独步全球！沈飞又...  | 1     | [2760, 188, 5, 148... | 15                  |\n",
      "| 以色列警告称如果...  | 1     | [66, 97, 82, 28, 8... | 16                  |\n",
      "| 与歼20竞标的“雪...   | 1     | [72, 989, 173, 148... | 13                  |\n",
      "| 观网小编进军营！...  | 1     | [2767, 124, 2768, ... | 9                   |\n",
      "| 一个月近900人死于... | 1     | [1492, 342, 2772, ... | 14                  |\n",
      "| 美国声称“没有输...   | 1     | [32, 1493, 14, 64,... | 19                  |\n",
      "| 五角大楼怒了！昨...  | 1     | [1495, 992, 7, 5, ... | 16                  |\n",
      "| 与恶狗对峙，该怎...  | 1     | [72, 2784, 2785, 2... | 7                   |\n",
      "| 中国001航母和辽宁... | 1     | [24, 2786, 245, 23... | 9                   |\n",
      "| 以色列警告称如果...  | 1     | [66, 97, 82, 28, 8... | 16                  |\n",
      "| 图-160战略轰炸机...  | 1     | [578, 80, 2787, 99... | 12                  |\n",
      "| 美国航母即使被导...  | 1     | [32, 245, 2789, 20... | 21                  |\n",
      "| 叙利亚政府军缴获...  | 1     | [180, 994, 995, 27... | 18                  |\n",
      "| ...                  | ...   | ...                   | ...                 |\n",
      "+----------------------+-------+-----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "print(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dev=data_bundle.get_dataset(name='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------+-----------------------+---------------------+\n",
      "| text                 | label | description_words     | description_seq_len |\n",
      "+----------------------+-------+-----------------------+---------------------+\n",
      "| 京城最值得你来场...  | 7     | [1927, 42, 357, 8,... | 9                   |\n",
      "| 发酵床的垫料种类...  | 7     | [7112, 7113, 4, 71... | 12                  |\n",
      "| 上联：黄山黄河黄...  | 7     | [39, 6, 7116, 7117... | 11                  |\n",
      "| 林徽因什么理由拒...  | 7     | [7120, 17, 603, 82... | 12                  |\n",
      "| 黄杨木是什么树？...  | 7     | [7124, 10, 17, 910... | 5                   |\n",
      "| 上联：草根登上星...  | 7     | [39, 6, 7125, 1463... | 11                  |\n",
      "| 什么是超写实绘画...  | 7     | [17, 10, 406, 7127... | 6                   |\n",
      "| 松涛听雨莺婉转，...  | 7     | [7129, 7130, 7131,... | 6                   |\n",
      "| 上联：老子骑牛读...  | 7     | [39, 6, 7132, 7133... | 10                  |\n",
      "| 上联：山水醉人何...  | 7     | [39, 6, 859, 2648,... | 11                  |\n",
      "| 国画山水，如何读...  | 7     | [7134, 859, 2, 18,... | 6                   |\n",
      "| 一元硬币是这种，...  | 7     | [7136, 7137, 10, 2... | 12                  |\n",
      "| 有哪些让人感动的...  | 7     | [11, 34, 78, 31, 2... | 9                   |\n",
      "| 上联，绿竹引清风...  | 7     | [39, 2, 7141, 2429... | 10                  |\n",
      "| 赵宁安作品欣赏       | 7     | [7142, 7143, 1179,... | 4                   |\n",
      "| 夕阳无语燕归愁，...  | 7     | [2589, 7144, 7145,... | 8                   |\n",
      "| 上联：山水醉人何...  | 7     | [39, 6, 859, 2648,... | 11                  |\n",
      "| 上联：上班为下班...  | 7     | [39, 6, 2555, 92, ... | 10                  |\n",
      "| 下联:夕陽西下已黄... | 7     | [37, 196, 7147, 71... | 12                  |\n",
      "| 初夏方好，小荷初...  | 7     | [2651, 7152, 2, 71... | 16                  |\n",
      "| 谢娜为李浩菲澄清...  | 4     | [282, 92, 7157, 71... | 16                  |\n",
      "| 谢娜曾为他与主办...  | 4     | [282, 1317, 92, 48... | 17                  |\n",
      "| 中国网红竟红到美...  | 4     | [24, 7163, 7164, 3... | 13                  |\n",
      "| 赵丽颖很久没有登...  | 4     | [415, 7168, 11, 14... | 18                  |\n",
      "| 因戴一个眼镜更改...  | 4     | [7174, 40, 1435, 1... | 17                  |\n",
      "| 后来的我们，抢先...  | 4     | [414, 4, 108, 2, 2... | 6                   |\n",
      "| 超级英雄演员颜值...  | 4     | [2652, 122, 656, 4... | 16                  |\n",
      "| 《无限歌谣季》热...  | 4     | [29, 1425, 7177, 1... | 15                  |\n",
      "| 张靓颖透露右耳已...  | 4     | [1620, 2131, 1622,... | 18                  |\n",
      "| 成龙改口决定不裸...  | 4     | [7183, 7184, 1349,... | 18                  |\n",
      "| 五大“出轨“女明...    | 4     | [770, 14, 7188, 14... | 13                  |\n",
      "| ...                  | ...   | ...                   | ...                 |\n",
      "+----------------------+-------+-----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "print(data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padding_words(data):\n",
    "    for i in range(len(data)):\n",
    "        if data[i]['description_seq_len'] <= max_sentence_length:\n",
    "            padding = [0] * (max_sentence_length - data[i]['description_seq_len'])\n",
    "            data[i]['description_words'] += padding\n",
    "        else:\n",
    "            pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train=data_bundle.get_dataset('train')\n",
    "data_train=padding_words(data_train)\n",
    "data_test=data_bundle.get_dataset('dev')\n",
    "data_test=padding_words(data_test)\n",
    "data_train.apply(lambda x: len(x['description_words']), new_field_name='description_seq_len')\n",
    "data_test.apply(lambda x: len(x['description_words']), new_field_name='description_seq_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------------------+---------------------+--------------------------+-----------+\n",
       "| text                 | description_seq_len | description_word_seq     | label_seq |\n",
       "+----------------------+---------------------+--------------------------+-----------+\n",
       "| 京城最值得你来场...  | 32                  | [1927, 42, 357, 8, 71... | 7         |\n",
       "| 发酵床的垫料种类...  | 32                  | [7112, 7113, 4, 7114,... | 7         |\n",
       "| 上联：黄山黄河黄...  | 32                  | [39, 6, 7116, 7117, 7... | 7         |\n",
       "| 林徽因什么理由拒...  | 32                  | [7120, 17, 603, 824, ... | 7         |\n",
       "| 黄杨木是什么树？...  | 32                  | [7124, 10, 17, 910, 3... | 7         |\n",
       "| 上联：草根登上星...  | 32                  | [39, 6, 7125, 1463, 7... | 7         |\n",
       "| 什么是超写实绘画...  | 32                  | [17, 10, 406, 7127, 7... | 7         |\n",
       "| 松涛听雨莺婉转，...  | 32                  | [7129, 7130, 7131, 2,... | 7         |\n",
       "| 上联：老子骑牛读...  | 32                  | [39, 6, 7132, 7133, 1... | 7         |\n",
       "| 上联：山水醉人何...  | 32                  | [39, 6, 859, 2648, 26... | 7         |\n",
       "| 国画山水，如何读...  | 32                  | [7134, 859, 2, 18, 21... | 7         |\n",
       "| 一元硬币是这种，...  | 32                  | [7136, 7137, 10, 259,... | 7         |\n",
       "| 有哪些让人感动的...  | 32                  | [11, 34, 78, 31, 2524... | 7         |\n",
       "| 上联，绿竹引清风...  | 32                  | [39, 2, 7141, 2429, 2... | 7         |\n",
       "| 赵宁安作品欣赏       | 32                  | [7142, 7143, 1179, 41... | 7         |\n",
       "| 夕阳无语燕归愁，...  | 32                  | [2589, 7144, 7145, 2,... | 7         |\n",
       "| 上联：山水醉人何...  | 32                  | [39, 6, 859, 2648, 26... | 7         |\n",
       "| 上联：上班为下班...  | 32                  | [39, 6, 2555, 92, 714... | 7         |\n",
       "| 下联:夕陽西下已黄... | 32                  | [37, 196, 7147, 7148,... | 7         |\n",
       "| 初夏方好，小荷初...  | 32                  | [2651, 7152, 2, 7153,... | 7         |\n",
       "| 谢娜为李浩菲澄清...  | 32                  | [282, 92, 7157, 7158,... | 4         |\n",
       "| 谢娜曾为他与主办...  | 32                  | [282, 1317, 92, 48, 7... | 4         |\n",
       "| 中国网红竟红到美...  | 32                  | [24, 7163, 7164, 32, ... | 4         |\n",
       "| 赵丽颖很久没有登...  | 32                  | [415, 7168, 11, 1463,... | 4         |\n",
       "| 因戴一个眼镜更改...  | 32                  | [7174, 40, 1435, 110,... | 4         |\n",
       "| 后来的我们，抢先...  | 32                  | [414, 4, 108, 2, 2195... | 4         |\n",
       "| 超级英雄演员颜值...  | 32                  | [2652, 122, 656, 452,... | 4         |\n",
       "| 《无限歌谣季》热...  | 32                  | [29, 1425, 7177, 1418... | 4         |\n",
       "| 张靓颖透露右耳已...  | 32                  | [1620, 2131, 1622, 37... | 4         |\n",
       "| 成龙改口决定不裸...  | 32                  | [7183, 7184, 1349, 71... | 4         |\n",
       "| 五大“出轨“女明...    | 32                  | [770, 14, 7188, 14, 1... | 4         |\n",
       "| ...                  | ...                 | ...                      | ...       |\n",
       "+----------------------+---------------------+--------------------------+-----------+"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.rename_field(\"description_words\",\"description_word_seq\")\n",
    "data_train.rename_field(\"label\",\"label_seq\")\n",
    "data_test.rename_field(\"description_words\",\"description_word_seq\")\n",
    "data_test.rename_field(\"label\",\"label_seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------------------+---------------------+--------------------------+-----------+\n",
       "| text                 | description_seq_len | description_word_seq     | label_seq |\n",
       "+----------------------+---------------------+--------------------------+-----------+\n",
       "| 京城最值得你来场...  | 32                  | [1927, 42, 357, 8, 71... | 7         |\n",
       "| 发酵床的垫料种类...  | 32                  | [7112, 7113, 4, 7114,... | 7         |\n",
       "| 上联：黄山黄河黄...  | 32                  | [39, 6, 7116, 7117, 7... | 7         |\n",
       "| 林徽因什么理由拒...  | 32                  | [7120, 17, 603, 824, ... | 7         |\n",
       "| 黄杨木是什么树？...  | 32                  | [7124, 10, 17, 910, 3... | 7         |\n",
       "| 上联：草根登上星...  | 32                  | [39, 6, 7125, 1463, 7... | 7         |\n",
       "| 什么是超写实绘画...  | 32                  | [17, 10, 406, 7127, 7... | 7         |\n",
       "| 松涛听雨莺婉转，...  | 32                  | [7129, 7130, 7131, 2,... | 7         |\n",
       "| 上联：老子骑牛读...  | 32                  | [39, 6, 7132, 7133, 1... | 7         |\n",
       "| 上联：山水醉人何...  | 32                  | [39, 6, 859, 2648, 26... | 7         |\n",
       "| 国画山水，如何读...  | 32                  | [7134, 859, 2, 18, 21... | 7         |\n",
       "| 一元硬币是这种，...  | 32                  | [7136, 7137, 10, 259,... | 7         |\n",
       "| 有哪些让人感动的...  | 32                  | [11, 34, 78, 31, 2524... | 7         |\n",
       "| 上联，绿竹引清风...  | 32                  | [39, 2, 7141, 2429, 2... | 7         |\n",
       "| 赵宁安作品欣赏       | 32                  | [7142, 7143, 1179, 41... | 7         |\n",
       "| 夕阳无语燕归愁，...  | 32                  | [2589, 7144, 7145, 2,... | 7         |\n",
       "| 上联：山水醉人何...  | 32                  | [39, 6, 859, 2648, 26... | 7         |\n",
       "| 上联：上班为下班...  | 32                  | [39, 6, 2555, 92, 714... | 7         |\n",
       "| 下联:夕陽西下已黄... | 32                  | [37, 196, 7147, 7148,... | 7         |\n",
       "| 初夏方好，小荷初...  | 32                  | [2651, 7152, 2, 7153,... | 7         |\n",
       "| 谢娜为李浩菲澄清...  | 32                  | [282, 92, 7157, 7158,... | 4         |\n",
       "| 谢娜曾为他与主办...  | 32                  | [282, 1317, 92, 48, 7... | 4         |\n",
       "| 中国网红竟红到美...  | 32                  | [24, 7163, 7164, 32, ... | 4         |\n",
       "| 赵丽颖很久没有登...  | 32                  | [415, 7168, 11, 1463,... | 4         |\n",
       "| 因戴一个眼镜更改...  | 32                  | [7174, 40, 1435, 110,... | 4         |\n",
       "| 后来的我们，抢先...  | 32                  | [414, 4, 108, 2, 2195... | 4         |\n",
       "| 超级英雄演员颜值...  | 32                  | [2652, 122, 656, 452,... | 4         |\n",
       "| 《无限歌谣季》热...  | 32                  | [29, 1425, 7177, 1418... | 4         |\n",
       "| 张靓颖透露右耳已...  | 32                  | [1620, 2131, 1622, 37... | 4         |\n",
       "| 成龙改口决定不裸...  | 32                  | [7183, 7184, 1349, 71... | 4         |\n",
       "| 五大“出轨“女明...    | 32                  | [770, 14, 7188, 14, 1... | 4         |\n",
       "| ...                  | ...                 | ...                      | ...       |\n",
       "+----------------------+---------------------+--------------------------+-----------+"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.set_input(\"description_word_seq\")\n",
    "data_test.set_input(\"description_word_seq\")\n",
    "data_train.set_target(\"label_seq\")\n",
    "data_test.set_target(\"label_seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset processed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"dataset processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, channel_size):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "\n",
    "        self.channel_size = channel_size\n",
    "        self.maxpool = nn.Sequential(\n",
    "            nn.ConstantPad1d(padding=(0, 1), value=0),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.channel_size\n",
    "                      , self.channel_size\n",
    "                      , kernel_size=3, padding=1),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.channel_size, self.channel_size\n",
    "                      , kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shortcut = self.maxpool(x)\n",
    "        x = self.conv(x_shortcut)\n",
    "        x = x + x_shortcut\n",
    "        return x\n",
    "\n",
    "\n",
    "class DPCNN(nn.Module):\n",
    "    def __init__(self,max_features\n",
    "                 ,word_embedding_dimension\n",
    "                 ,max_sentence_length\n",
    "                 ,num_classes):\n",
    "        super(DPCNN, self).__init__()\n",
    "        self.max_features = max_features\n",
    "        self.embed_size = word_embedding_dimension\n",
    "        self.maxlen = max_sentence_length\n",
    "        self.num_classes=num_classes\n",
    "        self.channel_size = 250\n",
    "\n",
    "        self.embedding = nn.Embedding(self.max_features, self.embed_size)\n",
    "        torch.nn.init.normal_(self.embedding.weight.data,mean=0,std=0.01)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "\n",
    "        # region embedding\n",
    "        self.region_embedding = nn.Sequential(\n",
    "            nn.Conv1d(self.embed_size, self.channel_size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.channel_size, self.channel_size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.channel_size, self.channel_size, kernel_size=3, padding=1),\n",
    "        )\n",
    "        \n",
    "        self.seq_len = self.maxlen\n",
    "        resnet_block_list = []\n",
    "        while (self.seq_len > 2):\n",
    "            resnet_block_list.append(ResnetBlock(self.channel_size))\n",
    "            self.seq_len = self.seq_len // 2\n",
    "        #         print('seqlen{}'.format(self.seq_len))\n",
    "        self.resnet_layer = nn.Sequential(*resnet_block_list)\n",
    "        #这是fc\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.channel_size*self.seq_len, self.num_classes),\n",
    "            nn.BatchNorm1d(self.num_classes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.num_classes, self.num_classes)\n",
    "         )\n",
    "    def forward(self, description_word_seq):\n",
    "        x = self.embedding(description_word_seq)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.region_embedding(x)\n",
    "        x = self.conv_block(x)\n",
    "        x = self.resnet_layer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        output = self.fc(x)\n",
    "        return {'output': output}\n",
    "    def predict(self, description_word_seq):\n",
    "        \"\"\"\n",
    "        :param word_seq: torch.LongTensor, [batch_size, seq_len]\n",
    "        :return predict: dict of torch.LongTensor, [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        output = self(description_word_seq)\n",
    "        _, predict = output['output'].max(dim=1)\n",
    "        return {'predict': predict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding_dimension = 300\n",
    "num_classes = len(lb.classes_)\n",
    "pickle_path = 'result/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=DPCNN(max_features=len(vocab)\n",
    "            ,word_embedding_dimension=word_embedding_dimension\n",
    "            ,max_sentence_length = max_sentence_length,num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPCNN(\n",
      "  (embedding): Embedding(8197, 300)\n",
      "  (region_embedding): Sequential(\n",
      "    (0): Conv1d(300, 250, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv_block): Sequential(\n",
      "    (0): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(250, 250, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (3): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU()\n",
      "    (5): Conv1d(250, 250, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      "  (resnet_layer): Sequential(\n",
      "    (0): ResnetBlock(\n",
      "      (maxpool): Sequential(\n",
      "        (0): ConstantPad1d(padding=(0, 1), value=0)\n",
      "        (1): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv): Sequential(\n",
      "        (0): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv1d(250, 250, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (3): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv1d(250, 250, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (1): ResnetBlock(\n",
      "      (maxpool): Sequential(\n",
      "        (0): ConstantPad1d(padding=(0, 1), value=0)\n",
      "        (1): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv): Sequential(\n",
      "        (0): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv1d(250, 250, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (3): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv1d(250, 250, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (2): ResnetBlock(\n",
      "      (maxpool): Sequential(\n",
      "        (0): ConstantPad1d(padding=(0, 1), value=0)\n",
      "        (1): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv): Sequential(\n",
      "        (0): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv1d(250, 250, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (3): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv1d(250, 250, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): ResnetBlock(\n",
      "      (maxpool): Sequential(\n",
      "        (0): ConstantPad1d(padding=(0, 1), value=0)\n",
      "        (1): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv): Sequential(\n",
      "        (0): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv1d(250, 250, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (3): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv1d(250, 250, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=15, bias=True)\n",
      "    (1): BatchNorm1d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=15, out_features=15, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input fields after batch(if batch size is 2):\n",
      "\tdescription_word_seq: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 32]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\tlabel_seq: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "\n",
      "training epochs started 2020-11-18-11-18-52-209677\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=125.0), HTML(value='')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 1.16 seconds!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DPCNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ResnetBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluation on dev at Epoch 1/5. Step:25/125: \n",
      "\r",
      "AccuracyMetric: acc=0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 1.12 seconds!\n",
      "Evaluation on dev at Epoch 2/5. Step:50/125: \n",
      "AccuracyMetric: acc=0.05\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 1.16 seconds!\n",
      "Evaluation on dev at Epoch 3/5. Step:75/125: \n",
      "AccuracyMetric: acc=0.0975\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 1.22 seconds!\n",
      "Evaluation on dev at Epoch 4/5. Step:100/125: \n",
      "AccuracyMetric: acc=0.2475\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 1.16 seconds!\n",
      "Evaluation on dev at Epoch 5/5. Step:125/125: \n",
      "AccuracyMetric: acc=0.2975\n",
      "\n",
      "Reloaded the best model.\n",
      "\n",
      "In Epoch:5/Step:125, got best dev performance:\n",
      "AccuracyMetric: acc=0.2975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 5,\n",
       " 'best_eval': {'AccuracyMetric': {'acc': 0.2975}},\n",
       " 'best_step': 125,\n",
       " 'seconds': 107.47}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from fastNLP import Trainer\n",
    "from copy import deepcopy\n",
    "from fastNLP.core.losses import CrossEntropyLoss\n",
    "from fastNLP.core.metrics import AccuracyMetric\n",
    "from fastNLP.core.optimizer import Adam\n",
    "# from fastNLP.core.utils import \n",
    "\n",
    "\n",
    "# load model\n",
    "model=DPCNN(max_features=len(vocab),word_embedding_dimension=word_embedding_dimension,max_sentence_length = max_sentence_length,num_classes=num_classes)\n",
    "\n",
    "# define loss and metric\n",
    "loss = CrossEntropyLoss(pred=\"output\",target=\"label_seq\")\n",
    "metric = AccuracyMetric(pred=\"predict\", target=\"label_seq\")\n",
    "\n",
    "\n",
    "# train model with train_data,and val model witst_data\n",
    "# embedding=300 gaussian init，weight_decay=0.0001, lr=0.001，epoch=5\n",
    "trainer=Trainer(model=model,train_data=data_train,dev_data=data_test,loss=loss,metrics=metric,save_path='CD',batch_size=64,n_epochs=5,optimizer=Adam(lr=0.001, weight_decay=0.0001))\n",
    "trainer.train()\n",
    "\n",
    "# save pickle\n",
    "# save_pickle(model,pickle_path=pickle_path,file_name='new_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, channel_size):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "\n",
    "        self.channel_size = channel_size\n",
    "        self.maxpool = nn.Sequential(\n",
    "            nn.ConstantPad1d(padding=(0, 1), value=0),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.channel_size\n",
    "                      , self.channel_size\n",
    "                      , kernel_size=3, padding=1),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.channel_size, self.channel_size\n",
    "                      , kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shortcut = self.maxpool(x)\n",
    "        x = self.conv(x_shortcut)\n",
    "        x = x + x_shortcut\n",
    "        return x\n",
    "\n",
    "\n",
    "class DPCNN(nn.Module):\n",
    "    def __init__(self,max_features\n",
    "                 ,word_embedding_dimension\n",
    "                 ,max_sentence_length\n",
    "                 ,num_classes):\n",
    "        super(DPCNN, self).__init__()\n",
    "        self.max_features = max_features\n",
    "        self.embed_size = word_embedding_dimension\n",
    "        self.maxlen = max_sentence_length\n",
    "        self.num_classes=num_classes\n",
    "        self.channel_size = 250\n",
    "\n",
    "        self.embedding = nn.Embedding(self.max_features, self.embed_size)\n",
    "        torch.nn.init.normal_(self.embedding.weight.data,mean=0,std=0.01)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "\n",
    "        # region embedding\n",
    "        self.region_embedding = nn.Sequential(\n",
    "            nn.Conv1d(self.embed_size, self.channel_size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.channel_size, self.channel_size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.channel_size, self.channel_size, kernel_size=3, padding=1),\n",
    "        )\n",
    "        \n",
    "        self.seq_len = self.maxlen\n",
    "        resnet_block_list = []\n",
    "        while (self.seq_len > 2):\n",
    "            resnet_block_list.append(ResnetBlock(self.channel_size))\n",
    "            self.seq_len = self.seq_len // 2\n",
    "        #         print('seqlen{}'.format(self.seq_len))\n",
    "        self.resnet_layer = nn.Sequential(*resnet_block_list)\n",
    "        #这是fc\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.channel_size*self.seq_len, self.num_classes),\n",
    "            nn.BatchNorm1d(self.num_classes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.num_classes, self.num_classes)\n",
    "         )\n",
    "    def forward(self, description_word_seq):\n",
    "        x = self.embedding(description_word_seq)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.region_embedding(x)\n",
    "        x = self.conv_block(x)\n",
    "        x = self.resnet_layer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        output = self.fc(x)\n",
    "        return {'output': output}\n",
    "    def predict(self, description_word_seq):\n",
    "        \"\"\"\n",
    "        :param word_seq: torch.LongTensor, [batch_size, seq_len]\n",
    "        :return predict: dict of torch.LongTensor, [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        output = self(description_word_seq)\n",
    "        _, predict = output['output'].max(dim=1)\n",
    "        return {'predict': predict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# textCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, max_features\n",
    "                 ,word_embedding_dimension\n",
    "                 ,max_sentence_length\n",
    "                 ,num_classes):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.max_features = max_features\n",
    "        self.embed_size = word_embedding_dimension\n",
    "        self.maxlen = max_sentence_length\n",
    "        self.num_classes=num_classes\n",
    "        self.embedding = nn.Embedding(self.max_features, self.embed_size)\n",
    "#         if config.embedding_pretrained is not None:\n",
    "#             self.embedding = nn.Embedding\n",
    "#             .from_pretrained(config.embedding_pretrained, freeze=False)\n",
    "#         else:\n",
    "#             self.embedding = nn.Embedding(config.n_vocab,\n",
    "#                                           config.embed\n",
    "#                                           , padding_idx=config.n_vocab - 1)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(1, 256 , (k, self.embed_size)) for k in (2, 3, 4)])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(256* len((2, 3, 4)), self.num_classes)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, description_word_seq):\n",
    "        out = self.embedding(description_word_seq)\n",
    "#         print(out)\n",
    "        out = out.unsqueeze(1)\n",
    "        out = torch.cat([self.conv_and_pool(out, conv) for conv in self.convs], 1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return {'output': out}\n",
    "    \n",
    "    def predict(self, description_word_seq):\n",
    "        \"\"\"\n",
    "        :param word_seq: torch.LongTensor, [batch_size, seq_len]\n",
    "        :return predict: dict of torch.LongTensor, [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        output = self(description_word_seq)\n",
    "        _, predict = output['output'].max(dim=1)\n",
    "        return {'predict': predict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input fields after batch(if batch size is 2):\n",
      "\tdescription_word_seq: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 32]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\tlabel_seq: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "\n",
      "training epochs started 2020-11-18-12-48-10-695998\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=125.0), HTML(value='')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 0.88 seconds!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type TextCNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluation on dev at Epoch 1/5. Step:25/125: \n",
      "\r",
      "AccuracyMetric: acc=0.26\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 0.87 seconds!\n",
      "Evaluation on dev at Epoch 2/5. Step:50/125: \n",
      "AccuracyMetric: acc=0.31\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 0.91 seconds!\n",
      "Evaluation on dev at Epoch 3/5. Step:75/125: \n",
      "AccuracyMetric: acc=0.3575\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 1.18 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 4/5. Step:100/125: \n",
      "\r",
      "AccuracyMetric: acc=0.4\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 0.86 seconds!\n",
      "Evaluation on dev at Epoch 5/5. Step:125/125: \n",
      "AccuracyMetric: acc=0.4125\n",
      "\n",
      "Reloaded the best model.\n",
      "\n",
      "In Epoch:5/Step:125, got best dev performance:\n",
      "AccuracyMetric: acc=0.4125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 5,\n",
       " 'best_eval': {'AccuracyMetric': {'acc': 0.4125}},\n",
       " 'best_step': 125,\n",
       " 'seconds': 65.16}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from fastNLP import Trainer\n",
    "from copy import deepcopy\n",
    "from fastNLP.core.losses import CrossEntropyLoss\n",
    "from fastNLP.core.metrics import AccuracyMetric\n",
    "from fastNLP.core.optimizer import Adam\n",
    "# from fastNLP.core.utils import \n",
    "\n",
    "\n",
    "# load model\n",
    "model=TextCNN(max_features=len(vocab),word_embedding_dimension=word_embedding_dimension,max_sentence_length = max_sentence_length,num_classes=num_classes)\n",
    "\n",
    "# define loss and metric\n",
    "loss = CrossEntropyLoss(pred=\"output\",target=\"label_seq\")\n",
    "metric = AccuracyMetric(pred=\"predict\", target=\"label_seq\")\n",
    "\n",
    "\n",
    "# train model with train_data,and val model witst_data\n",
    "# embedding=300 gaussian init，weight_decay=0.0001, lr=0.001，epoch=5\n",
    "trainer=Trainer(model=model,train_data=data_train,dev_data=data_test,loss=loss,metrics=metric,save_path='CD',batch_size=64,n_epochs=5,optimizer=Adam(lr=0.001, weight_decay=0.0001))\n",
    "trainer.train()\n",
    "\n",
    "# save pickle\n",
    "# save_pickle(model,pickle_path=pickle_path,file_name='new_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TextRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# class Config(object):\n",
    "\n",
    "#     \"\"\"配置参数\"\"\"\n",
    "#     def __init__(self, dataset, embedding):\n",
    "#         self.model_name = 'TextRNN'\n",
    "#         self.train_path = dataset + '/data/train.txt'                                # 训练集\n",
    "#         self.dev_path = dataset + '/data/dev.txt'                                    # 验证集\n",
    "#         self.test_path = dataset + '/data/test.txt'                                  # 测试集\n",
    "#         self.class_list = [x.strip() for x in open(\n",
    "#             dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单\n",
    "#         self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表\n",
    "#         self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果\n",
    "#         self.log_path = dataset + '/log/' + self.model_name\n",
    "#         self.embedding_pretrained = torch.tensor(\n",
    "#             np.load(dataset + '/data/' + embedding)[\"embeddings\"].astype('float32'))\\\n",
    "#             if embedding != 'random' else None                                       # 预训练词向量\n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备\n",
    "\n",
    "#         self.dropout = 0.5                                              # 随机失活\n",
    "#         self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练\n",
    "#         self.num_classes = len(self.class_list)                         # 类别数\n",
    "#         self.n_vocab = 0                                                # 词表大小，在运行时赋值\n",
    "#         self.num_epochs = 10                                            # epoch数\n",
    "#         self.batch_size = 128                                           # mini-batch大小\n",
    "#         self.pad_size = 32                                              # 每句话处理成的长度(短填长切)\n",
    "#         self.learning_rate = 1e-3                                       # 学习率\n",
    "#         self.embed = self.embedding_pretrained.size(1)\\\n",
    "#             if self.embedding_pretrained is not None else 300           # 字向量维度, 若使用了预训练词向量，则维度统一\n",
    "#         self.hidden_size = 128                                          # lstm隐藏层\n",
    "#         self.num_layers = 2                                             # lstm层数\n",
    "\n",
    "\n",
    "# '''Recurrent Neural Network for Text Classification with Multi-Task Learning'''\n",
    "\n",
    "\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, max_features\n",
    "                 ,word_embedding_dimension\n",
    "                 ,max_sentence_length\n",
    "                 ,num_classes):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.max_features = max_features\n",
    "        self.embed_size = word_embedding_dimension\n",
    "        self.maxlen = max_sentence_length\n",
    "        self.num_classes=num_classes\n",
    "        self.embedding = nn.Embedding(self.max_features, self.embed_size)   \n",
    "        self.lstm = nn.LSTM(self.embed_size,\n",
    "                            128,\n",
    "                            2,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True,\n",
    "                            dropout=0.5)\n",
    "        self.fc = nn.Linear(128*2, num_classes)\n",
    "\n",
    "    def forward(self, description_word_seq):\n",
    "#         x, _ = description_word_seq\n",
    "        x= description_word_seq\n",
    "#         x=x.permute(1,0)\n",
    "#         print('=='*20)\n",
    "#         print(x.size())\n",
    "        out = self.embedding(x)  # [batch_size, seq_len, embeding]=[128, 32, 300]\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.fc(out[:, -1, :])  # 句子最后时刻的 hidden state\n",
    "        return {'output': out}\n",
    "    \n",
    "    def predict(self, description_word_seq):\n",
    "        \"\"\"\n",
    "        :param word_seq: torch.LongTensor, [batch_size, seq_len]\n",
    "        :return predict: dict of torch.LongTensor, [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        output = self(description_word_seq)\n",
    "        _, predict = output['output'].max(dim=1)\n",
    "        return {'predict': predict}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input fields after batch(if batch size is 2):\n",
      "\tdescription_word_seq: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 32]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\tlabel_seq: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "\n",
      "training epochs started 2020-11-18-12-11-32-423229\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=125.0), HTML(value='')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 1.23 seconds!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type TextRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluation on dev at Epoch 1/5. Step:25/125: \n",
      "\r",
      "AccuracyMetric: acc=0.1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 1.22 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 2/5. Step:50/125: \n",
      "\r",
      "AccuracyMetric: acc=0.05\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 1.45 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 3/5. Step:75/125: \n",
      "\r",
      "AccuracyMetric: acc=0.1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 1.59 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 4/5. Step:100/125: \n",
      "\r",
      "AccuracyMetric: acc=0.05\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 1.29 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 5/5. Step:125/125: \n",
      "\r",
      "AccuracyMetric: acc=0.055\n",
      "\n",
      "Reloaded the best model.\n",
      "\n",
      "In Epoch:1/Step:25, got best dev performance:\n",
      "AccuracyMetric: acc=0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 1,\n",
       " 'best_eval': {'AccuracyMetric': {'acc': 0.1}},\n",
       " 'best_step': 25,\n",
       " 'seconds': 79.03}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from fastNLP import Trainer\n",
    "from copy import deepcopy\n",
    "from fastNLP.core.losses import CrossEntropyLoss\n",
    "from fastNLP.core.metrics import AccuracyMetric\n",
    "from fastNLP.core.optimizer import Adam\n",
    "# from fastNLP.core.utils import \n",
    "\n",
    "\n",
    "# load model\n",
    "model=TextRNN(max_features=len(vocab),word_embedding_dimension=word_embedding_dimension,max_sentence_length = max_sentence_length,num_classes=num_classes)\n",
    "\n",
    "# define loss and metric\n",
    "loss = CrossEntropyLoss(pred=\"output\",target=\"label_seq\")\n",
    "metric = AccuracyMetric(pred=\"predict\", target=\"label_seq\")\n",
    "\n",
    "\n",
    "# train model with train_data,and val model witst_data\n",
    "# embedding=300 gaussian init，weight_decay=0.0001, lr=0.001，epoch=5\n",
    "trainer=Trainer(model=model,train_data=data_train,dev_data=data_test,loss=loss,metrics=metric,save_path='CD',batch_size=64,n_epochs=5,optimizer=Adam(lr=0.001, weight_decay=0.0001))\n",
    "trainer.train()\n",
    "\n",
    "# save pickle\n",
    "# save_pickle(model,pickle_path=pickle_path,file_name='new_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextRNN(\n",
      "  (embedding): Embedding(8197, 300)\n",
      "  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TextRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# class Config(object):\n",
    "\n",
    "#     \"\"\"配置参数\"\"\"\n",
    "#     def __init__(self, dataset, embedding):\n",
    "#         self.model_name = 'TextRCNN'\n",
    "#         self.train_path = dataset + '/data/train.txt'                                # 训练集\n",
    "#         self.dev_path = dataset + '/data/dev.txt'                                    # 验证集\n",
    "#         self.test_path = dataset + '/data/test.txt'                                  # 测试集\n",
    "#         self.class_list = [x.strip() for x in open(\n",
    "#             dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单\n",
    "#         self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表\n",
    "#         self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果\n",
    "#         self.log_path = dataset + '/log/' + self.model_name\n",
    "#         self.embedding_pretrained = torch.tensor(\n",
    "#             np.load(dataset + '/data/' + embedding)[\"embeddings\"].astype('float32'))\\\n",
    "#             if embedding != 'random' else None                                       # 预训练词向量\n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备\n",
    "\n",
    "#         self.dropout = 1.0                                              # 随机失活\n",
    "#         self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练\n",
    "#         self.num_classes = len(self.class_list)                         # 类别数\n",
    "#         self.n_vocab = 0                                                # 词表大小，在运行时赋值\n",
    "#         self.num_epochs = 10                                            # epoch数\n",
    "#         self.batch_size = 128                                           # mini-batch大小\n",
    "#         self.pad_size = 32                                              # 每句话处理成的长度(短填长切)\n",
    "#         self.learning_rate = 1e-3                                       # 学习率\n",
    "#         self.embed = self.embedding_pretrained.size(1)\\\n",
    "#             if self.embedding_pretrained is not None else 300           # 字向量维度, 若使用了预训练词向量，则维度统一\n",
    "#         self.hidden_size = 256                                          # lstm隐藏层\n",
    "#         self.num_layers = 1                                             # lstm层数\n",
    "\n",
    "\n",
    "'''Recurrent Convolutional Neural Networks for Text Classification'''\n",
    "\n",
    "\n",
    "class TextRCNN(nn.Module):\n",
    "    def __init__(self, max_features\n",
    "                 ,word_embedding_dimension\n",
    "                 ,max_sentence_length\n",
    "                 ,num_classes):\n",
    "        super(TextRCNN, self).__init__()\n",
    "        self.max_features = max_features\n",
    "        self.embed_size = word_embedding_dimension\n",
    "        self.maxlen = max_sentence_length\n",
    "        self.num_classes=num_classes\n",
    "        self.embedding = nn.Embedding(self.max_features, self.embed_size) \n",
    "        \n",
    "#         if config.embedding_pretrained is not None:\n",
    "#             self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)\n",
    "#         else:\n",
    "#             self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)\n",
    "            \n",
    "            \n",
    "        self.lstm = nn.LSTM(self.embed_size\n",
    "                            , 256\n",
    "                            , 1,\n",
    "                            bidirectional=True\n",
    "                            , batch_first=True\n",
    "                            , dropout=0.5)\n",
    "        self.maxpool = nn.MaxPool1d(32)\n",
    "        self.fc = nn.Linear(256 * 2 + self.embed_size\n",
    "                            , num_classes)\n",
    "\n",
    "    def forward(self, description_word_seq):\n",
    "        x= description_word_seq\n",
    "        embed = self.embedding(x)  # [batch_size, seq_len, embeding]=[64, 32, 64]\n",
    "        out, _ = self.lstm(embed)\n",
    "        out = torch.cat((embed, out), 2)\n",
    "        out = F.relu(out)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.maxpool(out).squeeze()\n",
    "        out = self.fc(out)\n",
    "        return {'output': out}\n",
    "    \n",
    "    def predict(self, description_word_seq):\n",
    "        \"\"\"\n",
    "        :param word_seq: torch.LongTensor, [batch_size, seq_len]\n",
    "        :return predict: dict of torch.LongTensor, [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        output = self(description_word_seq)\n",
    "        _, predict = output['output'].max(dim=1)\n",
    "        return {'predict': predict}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input fields after batch(if batch size is 2):\n",
      "\tdescription_word_seq: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 32]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\tlabel_seq: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "\n",
      "training epochs started 2020-11-18-12-20-00-435385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=125.0), HTML(value='')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 1.78 seconds!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type TextRCNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluation on dev at Epoch 1/5. Step:25/125: \n",
      "\r",
      "AccuracyMetric: acc=0.16\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 1.75 seconds!\n",
      "Evaluation on dev at Epoch 2/5. Step:50/125: \n",
      "AccuracyMetric: acc=0.2325\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 1.54 seconds!\n",
      "Evaluation on dev at Epoch 3/5. Step:75/125: \n",
      "AccuracyMetric: acc=0.2875\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 1.51 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 4/5. Step:100/125: \n",
      "\r",
      "AccuracyMetric: acc=0.41\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 1.53 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 5/5. Step:125/125: \n",
      "\r",
      "AccuracyMetric: acc=0.4025\n",
      "\n",
      "Reloaded the best model.\n",
      "\n",
      "In Epoch:4/Step:100, got best dev performance:\n",
      "AccuracyMetric: acc=0.41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 4,\n",
       " 'best_eval': {'AccuracyMetric': {'acc': 0.41}},\n",
       " 'best_step': 100,\n",
       " 'seconds': 102.82}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from fastNLP import Trainer\n",
    "from copy import deepcopy\n",
    "from fastNLP.core.losses import CrossEntropyLoss\n",
    "from fastNLP.core.metrics import AccuracyMetric\n",
    "from fastNLP.core.optimizer import Adam\n",
    "# from fastNLP.core.utils import \n",
    "\n",
    "\n",
    "# load model\n",
    "model=TextRCNN(max_features=len(vocab),word_embedding_dimension=word_embedding_dimension,max_sentence_length = max_sentence_length,num_classes=num_classes)\n",
    "\n",
    "# define loss and metric\n",
    "loss = CrossEntropyLoss(pred=\"output\",target=\"label_seq\")\n",
    "metric = AccuracyMetric(pred=\"predict\", target=\"label_seq\")\n",
    "\n",
    "\n",
    "# train model with train_data,and val model witst_data\n",
    "# embedding=300 gaussian init，weight_decay=0.0001, lr=0.001，epoch=5\n",
    "trainer=Trainer(model=model,train_data=data_train,dev_data=data_test,loss=loss,metrics=metric,save_path='CD',batch_size=64,n_epochs=5,optimizer=Adam(lr=0.001, weight_decay=0.0001))\n",
    "trainer.train()\n",
    "\n",
    "# save pickle\n",
    "# save_pickle(model,pickle_path=pickle_path,file_name='new_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextRCNN(\n",
      "  (embedding): Embedding(8197, 300)\n",
      "  (lstm): LSTM(300, 256, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (maxpool): MaxPool1d(kernel_size=32, stride=32, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=812, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TextRNN_Att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# class Config(object):\n",
    "\n",
    "#     \"\"\"配置参数\"\"\"\n",
    "#     def __init__(self, dataset, embedding):\n",
    "#         self.model_name = 'TextRNN_Att'\n",
    "#         self.train_path = dataset + '/data/train.txt'                                # 训练集\n",
    "#         self.dev_path = dataset + '/data/dev.txt'                                    # 验证集\n",
    "#         self.test_path = dataset + '/data/test.txt'                                  # 测试集\n",
    "#         self.class_list = [x.strip() for x in open(\n",
    "#             dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单\n",
    "#         self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表\n",
    "#         self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果\n",
    "#         self.log_path = dataset + '/log/' + self.model_name\n",
    "#         self.embedding_pretrained = torch.tensor(\n",
    "#             np.load(dataset + '/data/' + embedding)[\"embeddings\"].astype('float32'))\\\n",
    "#             if embedding != 'random' else None                                       # 预训练词向量\n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备\n",
    "\n",
    "#         self.dropout = 0.5                                              # 随机失活\n",
    "#         self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练\n",
    "#         self.num_classes = len(self.class_list)                         # 类别数\n",
    "#         self.n_vocab = 0                                                # 词表大小，在运行时赋值\n",
    "#         self.num_epochs = 10                                            # epoch数\n",
    "#         self.batch_size = 128                                           # mini-batch大小\n",
    "#         self.pad_size = 32                                              # 每句话处理成的长度(短填长切)\n",
    "#         self.learning_rate = 1e-3                                       # 学习率\n",
    "#         self.embed = self.embedding_pretrained.size(1)\\\n",
    "#             if self.embedding_pretrained is not None else 300           # 字向量维度, 若使用了预训练词向量，则维度统一\n",
    "#         self.hidden_size = 128                                          # lstm隐藏层\n",
    "#         self.num_layers = 2                                             # lstm层数\n",
    "#         self.hidden_size2 = 64\n",
    "\n",
    "\n",
    "'''Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification'''\n",
    "\n",
    "\n",
    "class TextRNN_Att(nn.Module):\n",
    "    def __init__(self, max_features\n",
    "                 ,word_embedding_dimension\n",
    "                 ,max_sentence_length\n",
    "                 ,num_classes):\n",
    "        super(TextRNN_Att, self).__init__()\n",
    "        self.max_features = max_features\n",
    "        self.embed_size = word_embedding_dimension\n",
    "        self.maxlen = max_sentence_length\n",
    "        self.num_classes=num_classes\n",
    "        self.embedding = nn.Embedding(self.max_features, self.embed_size) \n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embed_size\n",
    "                            , 128\n",
    "                            , 2,\n",
    "                            bidirectional=True\n",
    "                            , batch_first=True\n",
    "                            , dropout=0.5)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        # self.u = nn.Parameter(torch.Tensor(config.hidden_size * 2, config.hidden_size * 2))\n",
    "        self.w = nn.Parameter(torch.zeros(128 * 2))\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.fc1 = nn.Linear(128 * 2, 64)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, description_word_seq):\n",
    "        x= description_word_seq\n",
    "        emb = self.embedding(x)  # [batch_size, seq_len, embeding]=[128, 32, 300]\n",
    "        H, _ = self.lstm(emb)  # [batch_size, seq_len, hidden_size * num_direction]=[128, 32, 256]\n",
    "\n",
    "        M = self.tanh1(H)  # [128, 32, 256]\n",
    "        # M = torch.tanh(torch.matmul(H, self.u))\n",
    "        alpha = F.softmax(torch.matmul(M, self.w), dim=1).unsqueeze(-1)  # [128, 32, 1]\n",
    "        out = H * alpha  # [128, 32, 256]\n",
    "        out = torch.sum(out, 1)  # [128, 256]\n",
    "        out = F.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc(out)  # [128, 64]\n",
    "        return {'output': out}\n",
    "    \n",
    "    def predict(self, description_word_seq):\n",
    "        \"\"\"\n",
    "        :param word_seq: torch.LongTensor, [batch_size, seq_len]\n",
    "        :return predict: dict of torch.LongTensor, [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        output = self(description_word_seq)\n",
    "        _, predict = output['output'].max(dim=1)\n",
    "        return {'predict': predict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextRNN_Att(\n",
      "  (embedding): Embedding(8197, 300)\n",
      "  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (tanh1): Tanh()\n",
      "  (tanh2): Tanh()\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc): Linear(in_features=64, out_features=15, bias=True)\n",
      ")\n",
      "input fields after batch(if batch size is 2):\n",
      "\tdescription_word_seq: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 32]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\tlabel_seq: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "\n",
      "training epochs started 2020-11-18-12-29-10-850211\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=125.0), HTML(value='')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 1.3 seconds!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type TextRNN_Att. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluation on dev at Epoch 1/5. Step:25/125: \n",
      "\r",
      "AccuracyMetric: acc=0.05\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 1.34 seconds!\n",
      "Evaluation on dev at Epoch 2/5. Step:50/125: \n",
      "AccuracyMetric: acc=0.06\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 1.35 seconds!\n",
      "Evaluation on dev at Epoch 3/5. Step:75/125: \n",
      "AccuracyMetric: acc=0.1775\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 1.33 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 4/5. Step:100/125: \n",
      "\r",
      "AccuracyMetric: acc=0.2275\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, layout=Layout(flex='2'), max=7.0), HTML(value='')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 1.43 seconds!\n",
      "Evaluation on dev at Epoch 5/5. Step:125/125: \n",
      "AccuracyMetric: acc=0.3025\n",
      "\n",
      "Reloaded the best model.\n",
      "\n",
      "In Epoch:5/Step:125, got best dev performance:\n",
      "AccuracyMetric: acc=0.3025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 5,\n",
       " 'best_eval': {'AccuracyMetric': {'acc': 0.3025}},\n",
       " 'best_step': 125,\n",
       " 'seconds': 83.68}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from fastNLP import Trainer\n",
    "from copy import deepcopy\n",
    "from fastNLP.core.losses import CrossEntropyLoss\n",
    "from fastNLP.core.metrics import AccuracyMetric\n",
    "from fastNLP.core.optimizer import Adam\n",
    "# from fastNLP.core.utils import \n",
    "\n",
    "\n",
    "# load model\n",
    "model=TextRNN_Att(max_features=len(vocab),word_embedding_dimension=word_embedding_dimension,max_sentence_length = max_sentence_length,num_classes=num_classes)\n",
    "print(model)\n",
    "# define loss and metric\n",
    "loss = CrossEntropyLoss(pred=\"output\",target=\"label_seq\")\n",
    "metric = AccuracyMetric(pred=\"predict\", target=\"label_seq\")\n",
    "\n",
    "\n",
    "# train model with train_data,and val model witst_data\n",
    "# embedding=300 gaussian init，weight_decay=0.0001, lr=0.001，epoch=5\n",
    "trainer=Trainer(model=model,train_data=data_train,dev_data=data_test,loss=loss,metrics=metric,save_path='CD',batch_size=64,n_epochs=5,optimizer=Adam(lr=0.001, weight_decay=0.0001))\n",
    "trainer.train()\n",
    "\n",
    "# save pickle\n",
    "# save_pickle(model,pickle_path=pickle_path,file_name='new_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
